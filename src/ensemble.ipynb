{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my (JC) contribution to this project. I've implemented the code for splitting the dataset into seperate datasets that we can then use for evaulation, and hence scoring of the pre-trained models we're attempting to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize questions\n",
    "def naive_categorize_question(example):\n",
    "    question = example['question'].lower()\n",
    "    if 'date' in question:\n",
    "        return 'date'\n",
    "    if 'during' in question:\n",
    "        return 'during'\n",
    "    if 'how are' in question:\n",
    "        return 'how are'\n",
    "    if 'how big' in question or 'how large' in question:\n",
    "        return 'how big/size'\n",
    "    if 'how many' in question or 'how much' in question:\n",
    "        return 'how m/m'\n",
    "    if 'how old' in question:\n",
    "        return 'how old'\n",
    "    if 'what time' in question:\n",
    "        return 'what time'\n",
    "    if 'what' in question or 'which' in question:\n",
    "        return 'what'\n",
    "    if 'when' in question:\n",
    "        return 'when'\n",
    "    if 'where' in question:\n",
    "        return 'where'\n",
    "    if 'who' in question:\n",
    "        return 'who'\n",
    "    if 'whom' in question:\n",
    "        return 'whom'\n",
    "    if 'why' in question:\n",
    "        return 'why'\n",
    "    else:\n",
    "        return 'undefined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'what': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 6595\n",
      "}), 'where': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 464\n",
      "}), 'who': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 1151\n",
      "}), 'undefined': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 421\n",
      "}), 'how m/m': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 771\n",
      "}), 'during': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 172\n",
      "}), 'when': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 710\n",
      "}), 'date': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 60\n",
      "}), 'how old': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 21\n",
      "}), 'why': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 148\n",
      "}), 'how big/size': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 12\n",
      "}), 'what time': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 10\n",
      "}), 'how are': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 35\n",
      "})}\n",
      "Category 'what': 6595 questions\n",
      "Category 'where': 464 questions\n",
      "Category 'who': 1151 questions\n",
      "Category 'undefined': 421 questions\n",
      "Category 'how m/m': 771 questions\n",
      "Category 'during': 172 questions\n",
      "Category 'when': 710 questions\n",
      "Category 'date': 60 questions\n",
      "Category 'how old': 21 questions\n",
      "Category 'why': 148 questions\n",
      "Category 'how big/size': 12 questions\n",
      "Category 'what time': 10 questions\n",
      "Category 'how are': 35 questions\n"
     ]
    }
   ],
   "source": [
    "# Load the SQuAD dataset\n",
    "import json\n",
    "dataset = load_dataset('squad', split='validation')\n",
    "\n",
    "# Add a new field for question categories\n",
    "dataset = dataset.map(lambda entry: {\"category\": naive_categorize_question(entry)})\n",
    "\n",
    "# Split the dataset into categories\n",
    "categories = dataset.unique('category')\n",
    "categorized_datasets = {category: dataset.filter(lambda example: example['category'] == category) for category in categories}\n",
    "jsonfile = {}\n",
    "for cat in categorized_datasets:\n",
    "    catclasses = []\n",
    "    for row in categorized_datasets.get(cat):\n",
    "        catclasses.append(row)\n",
    "    jsonfile[cat] = catclasses\n",
    "\n",
    "with open(\"question_classes.txt\", \"w\") as f:\n",
    "    json.dump(jsonfile, f)\n",
    "\n",
    "# Print out the size of each categorized dataset\n",
    "for category, subset in categorized_datasets.items():\n",
    "    print(f\"Category '{category}': {len(subset)} questions\")\n",
    "\n",
    "# If you want to save these subsets, you can do so like this:\n",
    "# for category, subset in categorized_datasets.items():\n",
    "#     subset.to_csv(f'squad_{category}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherylkong/opt/anaconda3/envs/CS2109s/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-05 15:38:46.865895: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-cased-distilled-squad...\n",
      "creating QA pipeline\n",
      "Starting eval on category: what time\n",
      "Model: distilbert-base-cased-distilled-squad Category: what time\n",
      "Exact Match (EM): 80.00\n",
      "F1 Score: 87.33\n",
      "\n",
      "Evaluating distilbert-base-uncased-distilled-squad...\n",
      "creating QA pipeline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(…)distilled-squad/resolve/main/config.json: 100%|██████████| 451/451 [00:00<00:00, 49.1kB/s]\n",
      "model.safetensors: 100%|██████████| 265M/265M [00:35<00:00, 7.55MB/s] \n",
      "(…)squad/resolve/main/tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 21.1kB/s]\n",
      "(…)d-distilled-squad/resolve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 5.51MB/s]\n",
      "(…)tilled-squad/resolve/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 5.45MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting eval on category: what time\n",
      "Model: distilbert-base-uncased-distilled-squad Category: what time\n",
      "Exact Match (EM): 80.00\n",
      "F1 Score: 88.33\n",
      "\n",
      "Evaluating quangb1910128/bert-finetuned-squad...\n",
      "creating QA pipeline\n",
      "Starting eval on category: what time\n",
      "Model: quangb1910128/bert-finetuned-squad Category: what time\n",
      "Exact Match (EM): 90.00\n",
      "F1 Score: 93.33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines.pt_utils import KeyPairDataset\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"squad\")\n",
    "# Function to calculate metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Pretrained models to evaluate\n",
    "model_names = [\n",
    "\n",
    "    'distilbert-base-cased-distilled-squad',\n",
    "    'distilbert-base-uncased-distilled-squad',\n",
    "    'quangb1910128/bert-finetuned-squad'\n",
    "]\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "    # Create a QA pipeline\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model_name)\n",
    "\n",
    "    # for category in categorized_datasets.keys():\n",
    "    for category in ['what time']:\n",
    "        print(f\"Starting eval on category: {category}\")\n",
    "        dataset = categorized_datasets[category]\n",
    "        # Making predictions and evaluating\n",
    "        \n",
    "        predictions = []\n",
    "        for entry in dataset:\n",
    "            result = qa_pipeline(entry[\"question\"], entry[\"context\"])\n",
    "            pred = {'id': entry['id'], 'prediction_text': result['answer']}\n",
    "            predictions.append(pred)\n",
    "\n",
    "        ref_ds = dataset.select_columns(['id', 'answers'])\n",
    "        references = []\n",
    "        for entry in ref_ds:\n",
    "            answer = {'id': entry['id'], 'answers': entry['answers']}\n",
    "            references.append(answer)\n",
    "\n",
    "        results = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "        print(f\"Model: {model_name} Category: {category}\")\n",
    "        print(f\"Exact Match (EM): {results['exact_match']:.2f}\")\n",
    "        print(f\"F1 Score: {results['f1']:.2f}\\n\")\n",
    "\n",
    "# You would need to implement the compute_f1 function based on the SQuAD evaluation script or import it if available.\n",
    "# Also note that making predictions on the full dataset would be slow and resource-intensive;\n",
    "# you might want to batch the predictions or use a subset of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
