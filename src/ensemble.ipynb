{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my (JC) contribution to this project. I've implemented the code for splitting the dataset into seperate datasets that we can then use for evaulation, and hence scoring of the pre-trained models we're attempting to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to categorize questions\n",
    "def naive_categorize_question(example):\n",
    "    question = example['question'].lower()\n",
    "    if 'date' in question:\n",
    "        return 'date'\n",
    "    if 'during' in question:\n",
    "        return 'during'\n",
    "    if 'how are' in question:\n",
    "        return 'how are'\n",
    "    if 'how big' in question or 'how large' in question:\n",
    "        return 'how big/size'\n",
    "    if 'how many' in question or 'how much' in question:\n",
    "        return 'how m/m'\n",
    "    if 'how old' in question:\n",
    "        return 'how old'\n",
    "    if 'what time' in question:\n",
    "        return 'what time'\n",
    "    if 'what' in question or 'which' in question:\n",
    "        return 'what'\n",
    "    if 'when' in question:\n",
    "        return 'when'\n",
    "    if 'where' in question:\n",
    "        return 'where'\n",
    "    if 'who' in question:\n",
    "        return 'who'\n",
    "    if 'whom' in question:\n",
    "        return 'whom'\n",
    "    if 'why' in question:\n",
    "        return 'why'\n",
    "    else:\n",
    "        return 'undefined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'what': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 6595\n",
      "}), 'where': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 464\n",
      "}), 'who': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 1151\n",
      "}), 'undefined': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 421\n",
      "}), 'how m/m': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 771\n",
      "}), 'during': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 172\n",
      "}), 'when': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 710\n",
      "}), 'date': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 60\n",
      "}), 'how old': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 21\n",
      "}), 'why': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 148\n",
      "}), 'how big/size': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 12\n",
      "}), 'what time': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 10\n",
      "}), 'how are': Dataset({\n",
      "    features: ['id', 'title', 'context', 'question', 'answers', 'category'],\n",
      "    num_rows: 35\n",
      "})}\n",
      "Category 'what': 6595 questions\n",
      "Category 'where': 464 questions\n",
      "Category 'who': 1151 questions\n",
      "Category 'undefined': 421 questions\n",
      "Category 'how m/m': 771 questions\n",
      "Category 'during': 172 questions\n",
      "Category 'when': 710 questions\n",
      "Category 'date': 60 questions\n",
      "Category 'how old': 21 questions\n",
      "Category 'why': 148 questions\n",
      "Category 'how big/size': 12 questions\n",
      "Category 'what time': 10 questions\n",
      "Category 'how are': 35 questions\n"
     ]
    }
   ],
   "source": [
    "# Load the SQuAD dataset\n",
    "dataset = load_dataset('squad', split='validation')\n",
    "\n",
    "# Add a new field for question categories\n",
    "dataset = dataset.map(lambda entry: {\"category\": naive_categorize_question(entry)})\n",
    "\n",
    "# Split the dataset into categories\n",
    "categories = dataset.unique('category')\n",
    "categorized_datasets = {category: dataset.filter(lambda example: example['category'] == category) for category in categories}\n",
    "print(categorized_datasets)\n",
    "\n",
    "# Print out the size of each categorized dataset\n",
    "for category, subset in categorized_datasets.items():\n",
    "    print(f\"Category '{category}': {len(subset)} questions\")\n",
    "\n",
    "# If you want to save these subsets, you can do so like this:\n",
    "# for category, subset in categorized_datasets.items():\n",
    "#     subset.to_csv(f'squad_{category}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "bbc = AutoModelForQuestionAnswering.from_pretrained('bert-base-cased')\n",
    "bbc_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting eval on category: how are\n",
      "Model: distilbert-base-cased Category: how are\n",
      "Exact Match (EM): 0.00\n",
      "F1 Score: 4.91\n",
      "\n",
      "Starting eval on category: how are\n",
      "Model: distilbert-base-cased Category: how are\n",
      "Exact Match (EM): 0.00\n",
      "F1 Score: 4.91\n",
      "\n",
      "Evaluating albert-base-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 684/684 [00:00<00:00, 91.2kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 47.4M/47.4M [00:00<00:00, 56.6MB/s]\n",
      "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 760k/760k [00:00<00:00, 40.4MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.31M/1.31M [00:00<00:00, 5.33MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting eval on category: how are\n",
      "Model: albert-base-v2 Category: how are\n",
      "Exact Match (EM): 0.00\n",
      "F1 Score: 4.62\n",
      "\n",
      "Starting eval on category: how are\n",
      "Model: albert-base-v2 Category: how are\n",
      "Exact Match (EM): 0.00\n",
      "F1 Score: 4.62\n",
      "\n",
      "Evaluating bert-base-cased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting eval on category: how are\n",
      "Model: bert-base-cased Category: how are\n",
      "Exact Match (EM): 0.00\n",
      "F1 Score: 6.59\n",
      "\n",
      "Starting eval on category: how are\n",
      "Model: bert-base-cased Category: how are\n",
      "Exact Match (EM): 0.00\n",
      "F1 Score: 6.59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines.pt_utils import KeyPairDataset\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"squad\")\n",
    "# Function to calculate metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    \n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Pretrained models to evaluate\n",
    "model_names = [\n",
    "    'distilbert-base-cased',\n",
    "    'albert-base-v2',\n",
    "    'bert-base-cased'\n",
    "]\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    # Load the model and tokenizer\n",
    "    # model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Create a QA pipeline\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "    # for category in categorized_datasets.keys():\n",
    "    for i in ['how big/size', 'what time']:\n",
    "        print(f\"Starting eval on category: {category}\")\n",
    "        dataset = categorized_datasets[category]\n",
    "        # Making predictions and evaluating\n",
    "        \n",
    "        predictions = []\n",
    "        for entry in dataset:\n",
    "            result = qa_pipeline(entry[\"question\"], entry[\"context\"])\n",
    "            pred = {'id': entry['id'], 'prediction_text': result['answer']}\n",
    "            predictions.append(pred)\n",
    "\n",
    "        ref_ds = dataset.select_columns(['id', 'answers'])\n",
    "        references = []\n",
    "        for entry in ref_ds:\n",
    "            answer = {'id': entry['id'], 'answers': entry['answers']}\n",
    "            references.append(answer)\n",
    "\n",
    "        results = metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "        print(f\"Model: {model_name} Category: {category}\")\n",
    "        print(f\"Exact Match (EM): {results['exact_match']:.2f}\")\n",
    "        print(f\"F1 Score: {results['f1']:.2f}\\n\")\n",
    "\n",
    "# You would need to implement the compute_f1 function based on the SQuAD evaluation script or import it if available.\n",
    "# Also note that making predictions on the full dataset would be slow and resource-intensive;\n",
    "# you might want to batch the predictions or use a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.2803937792778015, 'start': 0, 'end': 7, 'answer': 'context'}\n"
     ]
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model=bbc, tokenizer=tokenizer)\n",
    "\n",
    "print(question_answerer(question=\"question\", context=\"context\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric(name: \"squad\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
      "Computes SQuAD scores (F1 and EM).\n",
      "Args:\n",
      "    predictions: List of question-answers dictionaries with the following key-values:\n",
      "        - 'id': id of the question-answer pair as given in the references (see below)\n",
      "        - 'prediction_text': the text of the answer\n",
      "    references: List of question-answers dictionaries with the following key-values:\n",
      "        - 'id': id of the question-answer pair (see above),\n",
      "        - 'answers': a Dict in the SQuAD dataset format\n",
      "            {\n",
      "                'text': list of possible texts for the answer, as a list of strings\n",
      "                'answer_start': list of start positions for the answer, as a list of ints\n",
      "            }\n",
      "            Note that answer_start values are not taken into account to compute the metric.\n",
      "Returns:\n",
      "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
      "    'f1': The F-score of predicted tokens versus the gold answer\n",
      "Examples:\n",
      "\n",
      "    >>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]\n",
      "    >>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]\n",
      "    >>> squad_metric = datasets.load_metric(\"squad\")\n",
      "    >>> results = squad_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'exact_match': 100.0, 'f1': 100.0}\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"squad\")\n",
    "print(metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
